---
title: "Variable frame rate-based data augmentation to handle speaking-style variability for automatic speaker verification"
collection: publications
permalink: /publication/2020-10-25-vfr
excerpt: 'Data augmentation for speaker verfication'
date: 2020-10-25
venue: 'INTERSPEECH'

---
The effects of speaking-style variability on automatic speaker verification were investigated using the UCLA Speaker Variability database which comprises multiple speaking styles per speaker. An x-vector/PLDA (probabilistic linear discriminant analysis) system was trained with the SRE and Switchboard databases with standard augmentation techniques and evaluated with utterances from the UCLA database. The equal error rate (EER) was low when enrollment and test utterances were of the same style (e.g., 0.98% and 0.57% for read and conversational speech, respectively), but it increased substantially when styles were mismatched between enrollment and test utterances. For instance, when enrolled with conversation utterances, the EER increased to 3.03%, 2.96% and 22.12% when tested on read, narrative, and pet-directed speech, respectively. To reduce the effect of style mismatch, we propose an entropy-based variable frame rate technique to artificially generate style-normalized representations for PLDA adaptation. The proposed system significantly improved performance. In the aforementioned conditions, the EERs improved to 2.69% (conversation – read), 2.27% (conversation – narrative), and 18.75% (pet-directed – read). Overall, the proposed technique performed comparably to multi-style PLDA adaptation without the need for training data in different speaking styles per speaker.

[Download paper here](https://www.isca-speech.org/archive/Interspeech_2020/pdfs/3006.pdf){:target="_blank"}

Recommended citation: Afshan, A., Guo, J., Park, S.J., Ravi, V., McCree, A., Alwan, A. (2020) Variable Frame Rate-Based Data Augmentation to Handle Speaking-Style Variability for Automatic Speaker Verification. Proc. Interspeech 2020, 4318-4322, DOI: 10.21437/Interspeech.2020-3006.
